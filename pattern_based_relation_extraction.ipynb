{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHfQMjDnlACs"
   },
   "source": [
    "# Pattern Based Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEpjx2qKnyBl"
   },
   "source": [
    "---\n",
    "## Information Extraction and Relation Extraction\n",
    "The zip archive contains 100 files, out of which 50 are plaintext documents and other 50 contain data structured as JSON.\n",
    "Each plaintext document contains a text description of a movie taken from the English version of Wikipedia, while each JSON document contains *gold-standard* labels (also called *reference* labels) stored as key-value pairs for the entities and relations for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64euw8hXygah"
   },
   "source": [
    "---\n",
    "\n",
    "Download and unarchive `movies.zip` from Blackboard and place it in the same location as this notebook or uncomment the code cell below to get the data in a directory called `movies` and also place it automatically in the same location as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm1emJILzF5K"
   },
   "source": [
    "---\n",
    "\n",
    "## Reading Data\n",
    "\n",
    "Place the unzipped `movies` directory in the same location as this notebook and run the following code cell to read the plaintext and JSON documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P5hzct-HzUvJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "documents = []   # store the text documents as a list of strings\n",
    "labels = []      # store the gold-standard labels as a list of dictionaries\n",
    "\n",
    "for idx in range(50):\n",
    "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.doc.txt'), encoding = \"utf8\") as f:\n",
    "    doc = f.read().strip()\n",
    "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.info.json'), encoding = \"utf8\") as f:\n",
    "    label = json.load(f)\n",
    "\n",
    "  documents.append(doc)\n",
    "  labels.append(label)\n",
    "\n",
    "assert len(documents) == 50\n",
    "assert len(labels) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPCKvyYFj0zG"
   },
   "outputs": [],
   "source": [
    "# Load the libraries which might be useful\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "899-kd7LmlFp"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Document Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dB8R43AklZxO"
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  tagged_sentences = []\n",
    "  # your code goes here\n",
    "  # tokenizing the document into sentences, with each sentence tokenized into words before post_tagging\n",
    "  for sentence in nltk.sent_tokenize(document):\n",
    "    tagged_sentences.append(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "  return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9sEQYa3TBDYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('received', 'VBD'),\n",
       " ('ten', 'JJ'),\n",
       " ('Oscar', 'NNP'),\n",
       " ('nominations', 'NNS'),\n",
       " ('(', '('),\n",
       " ('including', 'VBG'),\n",
       " ('Best', 'NNP'),\n",
       " ('Picture', 'NN'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('winning', 'VBG'),\n",
       " ('seven', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check output for Task 1\n",
    "ie_preprocess(documents[0])[-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgEPCLXmq8bC"
   },
   "source": [
    "## Task 2: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qHKjz5TKp5uM"
   },
   "outputs": [],
   "source": [
    "def find_named_entities(tagged_document):\n",
    "  named_entities = []\n",
    "  for sent in tagged_document:\n",
    "    for chunk in nltk.ne_chunk(sent, binary = True):\n",
    "      try:\n",
    "        if chunk.label() == \"NE\":   # checking for NE label to extract named entities\n",
    "          named_entities.append(' '.join(i[0] for i in chunk))\n",
    "      except:\n",
    "        pass   # pass if the chunk is not a tree but a tuple (where there is no named entity)\n",
    "  return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lnlqsKg7sk29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars',\n",
       " 'Star Wars',\n",
       " 'New Hope',\n",
       " 'American',\n",
       " 'George Lucas',\n",
       " 'Lucasfilm',\n",
       " 'Century Fox',\n",
       " 'Mark Hamill',\n",
       " 'Harrison Ford',\n",
       " 'Carrie Fisher']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check output for Task 2\n",
    "tagged_document = ie_preprocess(documents[0]) # pre-process the first document\n",
    "find_named_entities(tagged_document)[:10]     # display the first 10 named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmpuu0yvwI_X"
   },
   "source": [
    "## Task 3: Information / Relation Extraction (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function to extract paired relations based on given pattern like *directed* , or *produced*\n",
    "def get_rel(document, pattern):\n",
    "  relations = []\n",
    "  tagged_document = ie_preprocess(document)\n",
    "  # Define X, Y, and \\alpha\n",
    "  subjclass = 'NE'\n",
    "  objclass = 'NE'\n",
    "  # Filter relevant relations by matching the regexp pattern.\n",
    "  relfilter = lambda x: (x['subjclass'] == subjclass and\n",
    "                           pattern.match(x['filler']) and\n",
    "                           x['objclass'] == objclass)\n",
    "  for sent in tagged_document:\n",
    "    chunked_sent = nltk.ne_chunk(sent, binary = True)\n",
    "    #print(chunked_sent)\n",
    "  \n",
    "  # Group a chunk structure into a list of 'semi-relations'.\n",
    "    pairs = nltk.sem.relextract.tree2semi_rel(chunked_sent)\n",
    "\n",
    "# Convert 'semi-relations' into a dictionary which stores information \n",
    "# about the subject and object NEs plus the filler between them.\n",
    "    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs + [[[]]])\n",
    "    rels = list(filter(relfilter, reldicts))\n",
    "    \n",
    "    # variable to store all the relations extracted in all the sentences\n",
    "    relations += rels\n",
    "  return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yw8YQAr-wwFM"
   },
   "outputs": [],
   "source": [
    "# relation 1\n",
    "def directed_by(document):\n",
    "  pattern_1 = re.compile(r\".*(directed|director).*\")\n",
    "\n",
    "  # pattern to extract for cases where the word \"directed\" is not between two named entities \n",
    "    # for e.g. if the sentence starts with \"...movie made in 2015. Directed by Christopher Nolan\"\n",
    "  pattern_2 = re.compile(r'.*\\b(directed|Directed)\\b.*')\n",
    "  \n",
    "  # set of directors variable extracted from pattern 1  \n",
    "  directors = {\"\".join(re.split(r\"/[A-Z]+\", re.findall(r\"\\[NE: '(.+?)'\\]\", nltk.sem.relextract.rtuple(rel))[-1])) \n",
    "                    for rel in get_rel(document, pattern_1)}\n",
    "  \n",
    "  # extraction using pattern 2\n",
    "  for sent in nltk.sent_tokenize(document):\n",
    "    if len(re.findall(pattern_2, sent)) > 0: # checking if the pattern is present\n",
    "      # finding the named entities \n",
    "      tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "      chunked_sent = nltk.ne_chunk(tagged_sent, binary = True)\n",
    "      flag = 0\n",
    "      for chunk in chunked_sent:\n",
    "        if flag == 0 and type(chunk) == tuple and chunk[0] in [\"directed\", \"Directed\"]:\n",
    "          flag = 1\n",
    "          continue\n",
    "        # the first named entity after setting the flag is taken as the value (e.g. first name after \"Directed by ..\")\n",
    "        if flag == 1 and type(chunk) == nltk.tree.Tree:\n",
    "          directors.add(\" \".join([i[0] for i in chunk.leaves()]))\n",
    "          break\n",
    "  # returning the list\n",
    "  return list(directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation 2\n",
    "def produced_by(document):\n",
    "  pattern_1 = re.compile(r\".*(produced|producer).*\")\n",
    "\n",
    "# pattern to extract for cases where the word \"directed\" is not between two named entities \n",
    "    # for e.g. if the sentence starts with \"...movie made in 2015. Produced by Christopher Nolan\"\n",
    "  pattern_2 = re.compile(r'.*\\b(Produced|produced|producer|Producer)\\b.*')\n",
    "  # set of producers variable extracted from pattern 1  \n",
    "  producers = {\"\".join(re.split(r\"/[A-Z]+\", re.findall(r\"\\[NE: '(.+?)'\\]\", nltk.sem.relextract.rtuple(rel))[-1])) \n",
    "                    for rel in get_rel(document, pattern_1)}\n",
    "  # extraction using pattern 2\n",
    "  for sent in nltk.sent_tokenize(document):\n",
    "    if len(re.findall(pattern_2, sent)) > 0:    # checking if the pattern is present\n",
    "      # finding the named entities \n",
    "      tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "      chunked_sent = nltk.ne_chunk(tagged_sent, binary = True)\n",
    "      flag = 0\n",
    "      for chunk in chunked_sent:\n",
    "        if flag == 0 and type(chunk) == tuple and chunk[0] in [\"Produced\", \"produced\", \"producer\", \"Producer\"]:\n",
    "          flag = 1\n",
    "          continue\n",
    "        # the first named entity after setting the flag is taken as the value (e.g. first name after \"Produced by ..\")\n",
    "        if flag == 1 and type(chunk) == nltk.tree.Tree:\n",
    "          producers.add(\" \".join([i[0] for i in chunk.leaves()]))\n",
    "          break\n",
    "  # returning the list\n",
    "  return list(producers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation 3\n",
    "def written_by(document):\n",
    "  pattern_1 = re.compile(r\".*(written|writer|writes).*\")\n",
    "# pattern to extract for cases where the word \"directed\" is not between two named entities \n",
    "    # for e.g. if the sentence starts with \"...movie made in 2015. Written by Christopher Nolan\"\n",
    "  pattern_2 = re.compile(r'.*\\b(written|Written|writer|Writer|writes|Writes)\\b.*')\n",
    "  # set of producers variable extracted from pattern 1 \n",
    "  writers = {\"\".join(re.split(r\"/[A-Z]+\", re.findall(r\"\\[NE: '(.+?)'\\]\", nltk.sem.relextract.rtuple(rel))[-1])) \n",
    "                    for rel in get_rel(document, pattern_1)}\n",
    "  # extraction using pattern 2\n",
    "  for sent in nltk.sent_tokenize(document):\n",
    "    if len(re.findall(pattern_2, sent)) > 0:    # checking if the pattern is present\n",
    "      # finding the named entities\n",
    "      tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "      chunked_sent = nltk.ne_chunk(tagged_sent, binary = True)\n",
    "      flag = 0\n",
    "      for chunk in chunked_sent:\n",
    "        if flag == 0 and type(chunk) == tuple and chunk[0] in [\"written\", \"Written\", \"writer\", \"Writer\", \"writes\", \"Writes\"]:\n",
    "          flag = 1\n",
    "          continue\n",
    "        # the first named entity after setting the flag is taken as the value (e.g. first name after \"Written by ..\")\n",
    "        if flag == 1 and type(chunk) == nltk.tree.Tree:\n",
    "          writers.add(\" \".join([i[0] for i in chunk.leaves()]))\n",
    "          break\n",
    "  # returning the list\n",
    "  return list(writers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuJmr-eKvrQ3"
   },
   "source": [
    "---\n",
    "## Task 4: Information / Relation Extraction (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kncUM3pHvyAT"
   },
   "outputs": [],
   "source": [
    "def part_of(document):\n",
    "  # pattern to identify is installment of the movie is mentioned\n",
    "  pattern = re.compile(r'.*\\b(installment)\\b.*')\n",
    "  part = set()\n",
    "  for sent in nltk.sent_tokenize(document):\n",
    "    if len(re.findall(pattern, sent)) > 0: # cheking if the pattern is present in sentence\n",
    "      part.add(sent.split(\"installment\")[0].split()[-1] + \" part\")\n",
    "  return list(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIfQCd_Y1x5B"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Combining information in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BE_8ptxp-1y4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Directed by': ['George Lucas'],\n",
       " 'Written by': ['George Lucas'],\n",
       " 'Produced by': ['Lucasfilm'],\n",
       " 'Task 4': ['first part']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_info(document):\n",
    "    output = {\n",
    "    ##### EDIT BELOW THIS LINE #####\n",
    "    # For the relations you extract in Task 3, \n",
    "    # save the output in the appropriate key and delete rest of the keys.\n",
    "    \"Directed by\": directed_by(document),\n",
    "    \"Written by\": written_by(document),\n",
    "    \"Produced by\": produced_by(document),\n",
    "    # save the output from Task 4 here\n",
    "    \"Task 4\": part_of(document),\n",
    "\n",
    "    ##### EDIT ABOVE THIS LINE #####\n",
    "  }\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "# check output for the first document\n",
    "extract_info(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDMhFQq4fBnf"
   },
   "source": [
    "---\n",
    "## Task 6: Evaluation (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OvJ9OhnDe7ML"
   },
   "outputs": [],
   "source": [
    "def evaluate(labels, predictions):\n",
    "  assert len(predictions) == len(labels)\n",
    "\n",
    "  scores = {\n",
    "      'precision': 0.0, 'recall': 0.0, 'f1': 0.0\n",
    "  }\n",
    "\n",
    "  # calculate the precision, recall and f1 score over the information fields \n",
    "  # corresponding to Task 3 and store the result in the `scores` dict.\n",
    "  tp, fp, fn = 0, 0, 0\n",
    "  for i in range(len(predictions)):\n",
    "    label, pred = labels[i], predictions[i]\n",
    "    for key, pred_vals in pred.items():\n",
    "        try:\n",
    "            true_vals = label[key]  # calculation is done for the keys present in labels\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for v in pred_vals:\n",
    "            if v in true_vals:\n",
    "                tp += 1   # true positive if predicted value is present in true labels.\n",
    "            else:\n",
    "                fp += 1   # false positive if predicted value is NOT present in true labels.\n",
    "        for v in true_vals:\n",
    "            if v not in pred_vals:\n",
    "                fn += 1   # flase negative if true value NOT present in predicted value.\n",
    "  # calculating and storing the values\n",
    "  scores[\"precision\"] = round(tp / (tp + fp), 2)\n",
    "  scores[\"recall\"] = round(tp / (tp + fn), 2)\n",
    "  scores[\"f1\"] = round(2 * scores[\"precision\"] * scores[\"recall\"] / (scores[\"precision\"] + scores[\"recall\"]), 2)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CRxOd4dIfRu-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall    f1\n",
       "0       0.55    0.34  0.42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# calculate evaluation score across all the 50 documents\n",
    "extracted_infos = []\n",
    "for document in documents:\n",
    "  extracted_infos.append(extract_info(document))\n",
    "\n",
    "scores = evaluate(labels, extracted_infos)\n",
    "\n",
    "pd.DataFrame([scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agNQPVqG5aoS"
   },
   "source": [
    "---\n",
    "## Task 7: Challenges and Issues with the above Evaluation (II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfdKI5J-fF1g"
   },
   "source": [
    "---\n",
    "There are a number of challenges associated with the above evaluation. Some of them are as follows:\n",
    "> 1. True values in labels have different value than what is present in text. For instance, in document 1 labels, producer is given as \"Gary Kurtz\", but in the 01.doc.txt, producer is mentioned as \"Lucasfilm\", which is correctly extracted but doesn't match the truth value. Same is the case in few other documents as well like for document 03.doc.txt and 02.doc.txt where truth label values are not present in movie text.\n",
    "> 2. All the keys/relations are not present in labels, for instance writer is extracted for document 04.doc.txt, but the written by field is not present in corresponding json file.\n",
    "\n",
    "The evaluation with the above issues doen't give a proper idea of how the extraction performs, since such issues will negatively affect the performance metric, which works only if a exact value is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
